typora-copy-images-to: ../../images/writings

# pyspark 学习教程

> 准备翻译一下官方文档自己学习一下，很多地方不太懂。
>
> [2.1.1版本地址](http://spark.apache.org/docs/latest/programming-guide.html)
>
> [参考某大神翻译的1.5.1版本，大部分还是一样的，就直接引用](http://cholerae.com/2015/04/11/-%E7%BF%BB%E8%AF%91-Spark%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-Python%E7%89%88/  )

#### 概览

每一个spark的应用程序包括一个驱动程序，用于执行用户的mian函数以及在集群上运行各种的并行操作。spark主要抽象是弹性分布式数据集，也就是rdd，这是一个包含诸多元素，被划分到不同节点上进行并行处理的数据集合。RDD通过打开HDFS（其他文件系统，等等均可）的文件，在驱动程序中打开一个已有的集合或者由其他的RDD转换得到，用户可以要求spark将rdd持久化到内存中，这样就可以有效的在并行操作中进行复用，在节点发生错误时RDD就可以自动恢复了，极大地提高了容错性。

spark提供的另一个抽象是可以在并行操作中可以使用共享的变量。在默认的情况下，当spark将一个函数转成很多task在不同节点上运行的时候，对于所有在函数中使用的变量，每一个任务都会得到一个副本，有时，某一个变量需要在task或者驱动程序中共享。spark支持两种共享变量：

1. 广播变量，用来将一个值缓存到所有节点的内存中去

2. 累加器，只能用于累加，比如计数器的求和

   ​

#### 连接Spark

spark2.1.1支持几乎所有的python，jupyter notebook也可以，一些c类库比如 numpy也是可以使用的。

通过spark目录下的bin/spark-submit脚本可以在python中运行spark的应用。这个脚本会载入spark的JAVA/SCALA的库然后让你将应用提交到集群中。

你将一些spark的类 import到你的程序中。加入这一行：

```python
from pyspark import SparkContext, SparkConf
```

pysaprk 需要在驱动和worker中使用相同的Python镜像，它使用的是PATH环境变量中默认的python版本，你可以在环境变量中指定你想用来写spark的python版本，使用PYSPARK_PYTHON

```shell
$ PYSPARK_PYTHON=python3.5 bin/pyspark
```

#### 初始化Spark

在一个spark程序中要做的第一件事就是创建一个SparkContext对象来告诉spark如何连接一个集群。为了创建一个SparkContext，首先需要创建一个sparkconf对象，这个对象中包含应用的一些相关信息。

```
conf = SparkConf().setAppName(appName).setMaster(master)
sc = SparkContext(conf=conf)
```

1. appName参数是指在集群的UI中显示的你的应用的名称。

2. master是一个Spark,mesoa,yarn集群的URL，如果想在本地运行的话是local字符串，在实际使用中，当在集群中运行程序，一般不会把master写死在代码中，而是通过用[ `spark-submit`](http://spark.apache.org/docs/latest/submitting-applications.html)运行程序来获得这个参数，在本地进行单元测试时，仍然需要自顶传入‘local‘来运行spark的程序。

   ​

   ​

#### 使用命令行

在pyspark的命令行中，一个特殊的集成在解释器里的SparkContext变量已经建好了，为sc。创建自己的SparkContext不会起到什么作用，可以通过--master的命令参数来设置这个上下文链接的master主机，也可以通过 --master设置master的主机，也可以通过 --py-files参数传递一个逗号隔开的列表将python的.zip .egg或者.py文件添加到运行的路径中或者通过package参数传递一个逗号隔开的maven列表添加依赖，比如spark的包。任何额外的包含依赖包的都可以通过--repositorys参数添加进去。spark包的所有python依赖（在requirement.txt中）都要用pip自己安装。

比如使用四核运行 bin/pyspark 

```python
$ ./bin/pyspark --master local[4]
$ ./bin/pyspark --master local[4] --py-files code.py

```

在[IPython](http://ipython.org/)这个加强的Python解释器中运行PySpark也是可行的。PySpark可以在1.0.0或更高版本的IPython上运行。为了使用IPython，必须在运行bin/pyspark时将PYSPARK_DRIVER_PYTHON变量设置为ipython，就像这样：

```
$ PYSPARK_DRIVER_PYTHON=ipython ./bin/pyspark

```

你还可以通过设置PYSPARK_DRIVER_PYTHON_OPTS来自省定制ipython。比如，在运行[IPython Notebook](http://ipython.org/notebook.html)
时开启PyLab图形支持应该使用这条命令：

```
$ PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS="notebook --pylab inline" ./bin/pyspark
```



#### 弹性分布式数据集（RDD）

spark是以RDD为中心运行的。RDD是一个容错的，可以被并行操作的元素集合，创建一个RDD有两个方法：

1. 在驱动程序中并行化一个已经存在的集合

2. 从外部存储系统中引用一个数据集，这个存储系统可以是一个共享文件系统，比如HDFS，HBASE 或任意提供了hadoop输入格式的数据源

   ​

##### 并行化集合

并行化集合是通过在驱动程序中-个现有的迭代器或者集合上调用SparkContext的parallelize方法建立，为了创建一个能够并行操作的分布数据集，集合中的元素都会被拷贝，比如羡慕创建了一个包含1到5的并行化集合：

```python
>>> data = [i for i in range(1,100)]
>>> distData = sc.parallelize(data)
>>> distData
ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475
>>> 
>>> distData.reduce(lambda a,b:a+b)
4950

```

分布数据集（distData）被建立起来之后，就可以进行并行操作了。比如，我们可以调用`disData.reduce(lambda a, b: a+b)`来对元素进行叠加。在后文中我们会描述分布数据集上支持的操作。

并行集合的一个重要参数是将数据集划分成**分片**的数量。对每一个分片，Spark会在集群中运行一个对应的任务。典型情况下，集群中的每一个CPU将对应运行2-4个分片。一般情况下，Spark会根据当前集群的情况自行设定分片数量。但是，你也可以通过将第二个参数传递给parallelize方法(比如sc.parallelize(data, 10))来手动确定分片数量。注意：有些代码中会使用切片（slice，分片的同义词）这个术语来保持向下兼容性。

##### 来自外部数据

PySpark可以通过Hadoop支持的外部数据源（包括本地文件系统、HDFS、 Cassandra、HBase、[亚马逊S3](http://wiki.apache.org/hadoop/AmazonS3)等等）建立分布数据集。Spark支持文本文件、[序列文件](http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html)以及其他任何[Hadoop输入格式](http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html)文件。

通过文本文件创建RDD要使用SparkContext的textFile方法。这个方法会使用一个文件的URI（或本地文件路径，hdfs://、s3n://这样的URI等等）然后读入这个文件建立一个文本行的集合。以下是一个例子：

```
>>> distFile = sc.textFile("data.txt")

```

建立完成后distFile上就可以调用数据集操作了。比如，我们可以调用map和reduce操作来叠加所有文本行的长度，代码如下：

```
distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)

```

在Spark中读入文件时有几点要注意：

- 如果使用了本地文件路径时，要保证在worker节点上这个文件也能够通过这个路径访问。这点可以通过将这个文件拷贝到所有worker上或者使用网络挂载的共享文件系统来解决。

- 包括textFile在内的所有基于文件的Spark读入方法，都支持将文件夹、压缩文件、包含通配符的路径作为参数。比如，以下代码都是合法的：

  ```
  textFile("/my/directory")
  textFile("/my/directory/*.txt")
  textFile("/my/directory/*.gz")

  ```

- textFile方法也可以传入第二个可选参数来控制文件的分片数量。默认情况下，Spark会为文件的每一个块（在HDFS中块的大小默认是64MB）创建一个分片。但是你也可以通过传入一个更大的值来要求Spark建立更多的分片。注意，分片的数量绝不能小于文件块的数量。

除了文本文件之外，Spark的Python API还支持多种其他数据格式：

- SparkContext.wholeTextFiles能够读入包含多个小文本文件的目录，然后为每一个文件返回一个（文件名，内容)对。这是与textFile方法为每一个文本行返回一条记录相对应的。
- RDD.saveAsPickleFile和SparkContext.pickleFile支持将RDD以串行化的Python对象格式存储起来。串行化的过程中会以默认10个一批的数量批量处理。
- 序列文件和其他Hadoop输入输出格式。

##### 可写支持类型

PySpark序列文件支持利用Java作为中介载入一个键值对RDD，将可写类型转化成Java的基本类型，然后使用[Pyrolite](https://github.com/irmen/Pyrolite/)将java结果对象串行化。当将一个键值对RDD储存到一个序列文件中时PySpark将会运行上述过程的相反过程。首先将Python对象反串行化成Java对象，然后转化成可写类型。以下可写类型会自动转换：

| Writable Type   | Python Type |
| --------------- | ----------- |
| Text            | unicode str |
| IntWritable     | int         |
| FloatWritable   | float       |
| DoubleWritable  | float       |
| BooleanWritable | bool        |
| BytesWritable   | bytearray   |
| NullWritable    | None        |
| MapWritable     | dict        |

##### 保存和读取序列文件

和文本文件类似，序列文件可以通过指定路径来保存与读取。键值类型都可以自行指定，但是对于标准可写类型可以不指定。

```python
>>> rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, "a" * x ))
>>> rdd.saveAsSequenceFile("path/to/file")
>>> sorted(sc.sequenceFile("path/to/file").collect())
[(1, u'a'), (2, u'aa'), (3, u'aaa')]
```

#### RDD操作

RDD支持两类操作：

1. 转化操作：用于从已有的数据集转化产生新的数据集

2. 启动操作：用于在计算结束后向驱动程序返回结果

     在Spark中，所有的转化操作都是惰性求值的，就是说他们不会立刻真的计算出结果，相反，他们仅仅记录下了转换的操作对象。只有当一个启动操作被执行的时候，要向驱动程序返回结果的时候转化操作擦会真的开始计算。这样的设计可以使得spark的计算效率更加高效----比如，如果发现由map产生的数据集将会在reduce操作中被用到，之后返回的是reduce的最终结果而不是map产生的庞大的数据集。

     在默认情况下，每一个由转化操作得到的RDD都会在每次执行启动操作时重新计算生成。但是，你也可以通过调用`persist`(或`cache`)方法来将RDD**持久化**到内存中，这样Spark就可以在下次使用这个数据集时快速获得。Spark同样提供了对将RDD持久化到硬盘上或在多个节点间复制的支持。

##### 基本操作

为了演示RDD的基本操作，请看下面的简单的程序

```python
lines = sc.textFile("data.txt")
lineLengths = lines.map(lambda s: len(s))
totalLength = lineLengths.reduce(lambda a, b: a + b)
```

第一行定义了一个由外部文件产生的基本RDD。这个数据集不是从内存中载入的也不是由其他操作产生的；`lines`仅仅是一个指向文件的指针。第二行将`lineLengths`定义为`map`操作的结果。再强调一次，由于惰性求值的缘故，`lineLengths`并**不会**被立即计算得到。最后，我们运行了`reduce`操作，这是一个启动操作。从这个操作开始，Spark将计算过程划分成许多任务并在多机上运行，每台机器运行自己部分的map操作和reduce操作，最终将自己部分的运算结果返回给驱动程序。

如果我们希望以后重复使用`lineLengths`，只需在`reduce`前加入下面这行代码：

```python
lineLengths.persist()
#这条代码将使得lineLengths在第一次计算生成之后保存在内存中。
```

##### 向spark传递函数

Spark的API严重依赖于向驱动程序传递函数作为参数，有三种方法来传递函数作为参数：

1. lambda表达式，简单的函数可以直接写成一个lambda表达式（lambda表达式不支持多语句函数和无返回值函数）

2. 对于代码很长的函数，在spark函数调用中在本地用def定义。

3. 模块中的顶级函数

   ​

比如传递一个无法转化为lambda表达式的长函数，可以像以下代码这样：

```python
"MyScript.py"""
if __name__ == "__main__":
    def myFunc(s):
        words = s.split(" ")
        return len(words)

    sc = SparkContext(...)
    sc.textFile("file.txt").map(myFunc)

```

值得指出的是，也可以传递类实例中方法的引用（与单例对象相反），这种传递方法会将整个对象传递进去。比如

```python
class MyClass(object):
    def func(self, s):
        return s
    def doStuff(self, rdd):
        return rdd.map(self.func)
#在这里，如果我们创建了一个新的MyClass对象，然后对他调用doStaff方法，map会用到这个对象中func方法的引用，所以整个对象都要传递到集群中去。
```

还有另外一种相似的写法，访问外层对象的数据域会传递整个对象的引用：

```python
class MyClass(object):
    def __init__(self):
        self.field = "Hello"
    def doStuff(self, rdd):
        return rdd.map(lambda s: self.field + x)
```

此类问题最简单的避免方法就是，使用一个本地变量缓存一份这个数据域的拷贝，直接访问这个数据域：

```python
def doStuff(self, rdd):
    field = self.field
    return rdd.map(lambda s: field + x)
```

#### 了解闭包

关于Spark的一个更难的事情就是在跨集群执行代码时了解变量和方法的范围和生命周期。 在其范围之外修改变量的RDD操作可能是常见的混乱原因。 在下面的例子中，我们将看看使用foreach（）来增加计数器的代码，但是对于其他操作也可能会出现类似的问题。

例如：

考虑一个一般的RDD求和的问题，由于是否在不同的JVM上运行可能会有不同的表现，一个最常见的例子当我们以local模式运行spark时（--master = local[n]）和在集群中运行一个spark的程序。

（eg 使用spark-submit 给Yarn）

```python
counter = 0
rdd = sc.parallelize(data)

# Wrong: Don't do this!! 这样是错误的
def increment_counter(x):
    global counter
    counter += x
rdd.foreach(increment_counter)

print("Counter value: ", counter)
```

###### local 和 集群模式

上面代码的结果表现为未定义的，可能无法正常工作，为了执行任务，spark将RDD的操作处理为一个个的任务，每个任务会被一个执行器（executor）执行，在执行之前，spark会计算task任务的closure。这个闭包（closure）包含了执行器在计算时必须可见的变量和方法，closure是序列化的同事发送给每个executor。

发送给每个执行器的闭包中的变量现在都是副本，因此，当foreach函数中引入计数器时，它已经不再是驱动程序节点上的计数器了，在驱动程序的节点上还有一个计数器，但是这对于执行程序来说已经不再可见了，执行者只能看到序列化闭包的拷贝，因此，计数器的最终值仍然为0，因为计数器上的所有操作都引用了序列化闭包中的值。

在本地模式下，在某些情况下，foreach函数实际上将在与驱动程序相同的JVM内执行，并引用相同的原始计数器，并可能实际更新它。

为了确保在这些场景中明确定义的行为，应该使用累加器。 Spark中的累加器专门用于提供一种在集群中的工作节点上执行分割时安全更新变量的机制。本指南的“累加器”部分更详细地讨论了这些。

一般来说，闭包 构造比如循环或者本地定义的方法不应该用于全局状态的变化，spark并不能定义或者保证闭包外部的变量的突变。这样的代码在本地模式下可能会工作，但是这只是意外，并且这种代码在分布式模式下将不会按预期的方式运行。如果需要进行全局聚合，则使用累加器。

###### Printing elements of an RDD

另外一个常见的场景是尝试使用rdd.foreach（println）或rdd.map（println）打印RDD的元素。

在单个节点上，可能会按照预期打印或者输出，但是，在集群模式下，执行程序调用的stdout的输出也写入了执行器的stdout，所以不会在驱动程序中输出，因此驱动程序上的stdout不会显示这些！如果要想在驱动程序中打印所有的元素，可以先使用collect方法首先将RDD带到驱动程序的节点：

```python
rdd.collect().foreach(println)
#这可能会导致驱动程序用尽内存，因为collect()将整个RDD提取到了一台机子上，如果只需要打印几个元素，一个更安全的方法是使用take():
rdd.take(100).foreach(println)
```

#### 使用键值对

虽然大部分Spark的RDD操作都支持所有种类的对象，但是有少部分特殊的操作只能作用于键值对类型的RDD。这类操作中最常见的就是分布的`shuffle`操作，比如将元素通过键来分组或聚集计算。

在Python中，这类操作一般都会使用Python内建的元组类型，比如(1, 2)。它们会先简单地创建类似这样的元组，然后调用你想要的操作。

比如，以下对调用了`reduceByKey`操作,来统计每一文本行在文本文件中出现的次数：

```python
lines = sc.textFile("data.txt")
pairs = lines.map(lambda s: (s, 1))
counts = pairs.reduceByKey(lambda a, b: a + b)
```

我们还可以使用`counts.sortByKey()`，比如，当我们想将这些键值对按照字母表顺序排序，然后调用`counts.collect()`方法来将结果以对象列表的形式返回。

#### 转化操作

下面的表格列出了Spark支持的常用转化操作。详细参考([Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD), [Java](http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaRDD.html), [Python](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD), [R](http://spark.apache.org/docs/latest/api/R/index.html))

| Transformation                           | Meaning                                  |
| ---------------------------------------- | ---------------------------------------- |
| **map**(*func*)                          | Return a new distributed dataset formed by passing each element of the source through a function *func*.返回一个新的分布数据集，由原数据集元素经过func处理后的结果组成。 |
| rdd = sc.parallelize(["b", "a", "c"])  sorted(rdd.**map**(lambda x: (x, 1)).collect()) | ``[('a', 1), ('b', 1), ('c', 1)]``       |
| **filter**(*func*)                       | Return a new dataset formed by selecting those elements of the source on which *func* returns true.返回一个新的数据集由传给func返回True的原来的数据集组成 |
| rdd = sc.parallelize([1, 2, 3, 4,5])   rdd.**filter**(lambda x: x % 2 == 0).collect() | ``[2,4]``                                |
| **flatMap**(*func*)Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. | Similar to map, but each input item can be mapped to 0 or more output items (so *func* should return a Seq rather than a single item).与map类似，但是每个传入元素可能有0或多个返回值，func可以返回一个序列而不是一个值 |
| rdd = sc.parallelize([2, 3, 4])  sorted(rdd.**flatMap**(lambda x: range(1, x)).collect()) | ``[1, 1, 1, 2, 2, 3]``                   |
| **mapPartitions**(*func*)Return a new RDD by applying a function to each partition of this RDD. | Similar to map, but runs separately on each partition (block) of the RDD, so *func*must be of type Iterator<T> => Iterator<U> when running on an RDD of type T.类似map，但是RDD的每个分片都会分开独立运行，所以func的参数和返回值必须都是迭代器 |
| rdd = sc.parallelize([1, 2, 3, 4], 2)                            def f(iterator): yield sum(iterator) rdd.mapPartitions(f).collect() | ``[3, 7]``  #解释一下为什么运行的结果为[3,7]，分了两个分片，第一个是1,2 第二个是3,4 |
| **mapPartitionsWithIndex**(*func*)       | Similar to mapPartitions, but also provides *func* with an integer value representing the index of the partition, so *func* must be of type (Int, Iterator<T>) => Iterator<U> when running on an RDD of type T.类似mapParitions，但是func有两个参数，第一个是分片的序号，第二个是迭代器。返回值还是迭代器 |
| rdd = sc.parallelize([1, 2, 3, 4], 4)                           def f(splitIndex, iterator): yield splitIndex rdd.mapPartitionsWithIndex(f).sum() | 6                                        |
| **sample**(*withReplacement*, *fraction*, *seed*) | Sample a fraction *fraction* of the data, with or without replacement, using a given random number generator seed.使用提供的随机数种子取样，然后替换或不替换 |
|                                          |                                          |
| **union**(*otherDataset*)                | Return a new dataset that contains the union of the elements in the source dataset and the argument.返回新的数据集，包括原数据集和参数数据集的所有元素 |
| **intersection**(*otherDataset*)         | Return a new RDD that contains the intersection of elements in the source dataset and the argument.返回新数据集，是两个集的交集 |
| **distinct**([*numTasks*]))              | Return a new dataset that contains the distinct elements of the source dataset.返回新的集，包括原集中的不重复元素 |
| **groupByKey**([*numTasks*])             | When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. **Note:** If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using `reduceByKey` or `aggregateByKey` will yield much better performance. **Note:** By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional `numTasks` argument to set a different number of tasks.当用于键值对RDD时返回(键，值迭代器)对的数据集 |
| **reduceByKey**(*func*, [*numTasks*])    | When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function *func*, which must be of type (V,V) => V. Like in `groupByKey`, the number of reduce tasks is configurable through an optional second argument. |
| **aggregateByKey**(*zeroValue*)(*seqOp*, *combOp*, [*numTasks*]) | When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral "zero" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in `groupByKey`, the number of reduce tasks is configurable through an optional second argument.用于键值对RDD时返回（K，U）对集，对每一个Key的value进行聚集计算 |
| **sortByKey**([*ascending*], [*numTasks*]) | When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean `ascending` argument.用于键值对RDD时会返回RDD按键的顺序排序，升降序由第一个参数决定 |
| **join**(*otherDataset*, [*numTasks*])   | When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through `leftOuterJoin`, `rightOuterJoin`, and `fullOuterJoin`.用于键值对(K, V)和(K, W)RDD时返回(K, (V, W))对RDD |
| **cogroup**(*otherDataset*, [*numTasks*]) | When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called `groupWith`.用于两个键值对RDD时返回 |
| **cartesian**(*otherDataset*)            | When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).用于T和U类型RDD时返回(T, U)对类型键值对RDD |
| **pipe**(*command*, *[envVars]*)         | Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to its stdout are returned as an RDD of strings.通过shell命令管道处理每个RDD分片 |
| **coalesce**(*numPartitions*)            | Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.把RDD的分片数量降低到参数大小 |
| **repartition**(*numPartitions*)         | Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.重新打乱RDD中元素顺序并重新分片，数量由参数决定 |
| **repartitionAndSortWithinPartitions**(*partitioner*) | Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling `repartition` and then sorting within each partition because it can push the sorting down into the shuffle machinery.按照参数给定的分片器重新分片，同时每个分片内部按照键排序 |

#### Actions

The following table lists some of the common actions supported by Spark. Refer to the RDD API doc ([Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD), [Java](http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaRDD.html), [Python](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD), [R](http://spark.apache.org/docs/latest/api/R/index.html))

| Action                                   | Meaning                                  |
| ---------------------------------------- | ---------------------------------------- |
| **reduce**(*func*)                       | Aggregate the elements of the dataset using a function *func* (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.使用func进行聚集计算,func的参数是两个，返回值一个，两次func运行应当是完全解耦的，这样才能正确地并行运算 |
| **collect**()                            | Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.向驱动程序返回数据集的元素组成的数组 |
| **count**()                              | Return the number of elements in the dataset.返回数据集元素的数量 |
| **first**()                              | Return the first element of the dataset (similar to take(1)).返回数据集的第一个元素 |
| **take**(*n*)                            | Return an array with the first *n* elements of the dataset.返回前n个元素组成的数组 |
| **takeSample**(*withReplacement*, *num*, [*seed*]) | Return an array with a random sample of *num* elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.返回一个由原数据集中任意num个元素的suzuki，并且替换之 |
| **takeOrdered**(*n*, *[ordering]*)       | Return the first *n* elements of the RDD using either their natural order or a custom comparator.返回排序后的前n个元素 |
| **saveAsTextFile**(*path*)               | Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.将数据集的元素写成文本文件 |
| **saveAsSequenceFile**(*path*) (Java and Scala) | Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).将数据集的元素写成序列文件，这个API只能用于Java和Scala程序 |
| **saveAsObjectFile**(*path*) (Java and Scala) | Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using`SparkContext.objectFile()`.将数据集的元素使用Java的序列化特性写到文件中，这个API只能用于Java和Scala程序 |
| **countByKey**()                         | Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.只能用于键值对RDD，返回一个(K, int) hashmap，返回每个key的出现次数 |
| **foreach**(*func*)                      | Run a function *func* on each element of the dataset. This is usually done for side effects such as updating an [Accumulator](http://spark.apache.org/docs/latest/programming-guide.html#accumulators) or interacting with external storage systems. 对数据集的每个元素执行func, 通常用于完成一些带有副作用的函数，比如更新累加器（见下文）或与外部存储交互等 |

#### Shuffle操作

Spark内的某些操作会触发shuffle。 shuffle是Spark的重新分配数据的机制，因此它在分区之间的分组不同。 这通常涉及将数据复制到执行程序和机器上，从而使洗牌（shuffle）成为复杂而昂贵的操作。

##### 背景

为了理解在shuffle时到底发生了什么，我们可以参考 ``reduceByKey``操作。

``reduceByKey``操作生成了一个 <key, 所有value的元组>的新的RDD-对与所述键相关联的所有值执行reduce函数的关键字和结果。 挑战在于，并不是每个key的所有值都必须驻留在同一个分区上，或者甚至位于同一个机器上，但它们必须位于同一位置才能计算结果。

在spark中，数据通常不会跨区分布，以便在特定操作的必须的分区中。在一个计算任务中，单个任务会在单个分区上进行，因此，为了执行单个reduceByKey还原任务的所有数据，Spark需要执行多对多的操作。必须从所有的分区中找到所有的key的value，然后跨越分区汇总每个key的最终结果。这就是洗牌操作。

虽然新的shuffle后的data在每个分区中的元素都是确定性的，也就是分区内的他们的顺序，但是shuffle后的不是，如果想要在shuffle后仍然可以得到有序数据，那么可以使用下面的方法。

- `mapPartitions` 对每个分区排序, for example, `.sorted`
- `repartitionAndSortWithinPartitions` 有效地对分区进行分类，同时重新分区
- ``sortBy`` 生成全局有序的rdd

可能导致洗牌的操作包括重新分区操作，如重新分配和合并，[`repartition`](http://spark.apache.org/docs/latest/programming-guide.html#RepartitionLink) and [`coalesce`](http://spark.apache.org/docs/latest/programming-guide.html#CoalesceLink)，如ByKey操作（除了用于计数）比如groupByKey 和 reduceByKey，以及join操作例如[`cogroup`](http://spark.apache.org/docs/latest/programming-guide.html#CogroupLink) and [`join`](http://spark.apache.org/docs/latest/programming-guide.html#JoinLink) 。

##### 性能影响

shuffle操作是一项昂贵的操作，因为它涉及磁盘I / O，数据串行化和网络I / O。要组织shuffle的数据，Spark会生成一组任务 - ``map``任务以组织数据，以及一组reduce任务以进行汇总。这个命名法来自MapReduce，并不直接与Spark的``map``和``reduce``操作有关。

在内部，每个map任务的结果保存在内存中。然后，这些根据目标分区进行排序并写入单个文件。在reduce的方面，任务读取相关的排序后的块。

某些随机操作可能会消耗大量的堆内存，因为它们在传输之前或之后使用内存中的数据结构来组织记录。具体来说，``reduceByKey``和``aggregateByKey``在地图上创建这些结构，“ByKey”操作会在reduce方面生成这些结构。当数据不适合继续在内存中存储时，Spark会将这些表溢出到磁盘，从而导致磁盘I / O的额外开销。

随机播放还会在磁盘上生成大量的中间文件。从Spark 1.3开始，这些文件将被保留，直到相应的RDD不再使用并被垃圾回收。这样做，所以如果重新计算谱系，则不需要重新创建shuffle文件。如果应用程序保留对这些RDD的引用或GC不频繁启动，垃圾收集可能只会在很长一段时间之后发生。这意味着长时间运行的Spark作业可能会占用大量的磁盘空间。在配置Spark上下文时，由spark.local.dir配置参数指定临时存储目录。

可以通过调整各种配置参数来调整shuffle行为。请参阅“Spark配置指南”中的[Spark Configuration ](http://spark.apache.org/docs/latest/configuration.html)”部分。

#### RDD持久化

Spark的一个重要功能就是在将数据集**持久化**（或**缓存**）到内存中以便在多个操作中重复使用。当我们持久化一个RDD是，每一个节点将这个RDD的每一个分片计算并保存到内存中以便在下次对这个数据集（或者这个数据集衍生的数据集）的计算中可以复用。这使得接下来的计算过程速度能够加快（经常能加快超过十倍的速度）。缓存是加快迭代算法和快速交互过程速度的关键工具。

你可以通过调用`persist`或`cache`方法来标记一个想要持久化的RDD。在第一次被计算产生之后，它就会始终停留在节点的内存中。Spark的缓存是具有容错性的——如果RDD的任意一个分片丢失了，Spark就会依照这个RDD产生的转化过程自动重算一遍。

另外，每一个持久化的RDD都有一个可变的**存储级别**，这个级别使得用户可以改变RDD持久化的储存位置。比如，你可以将数据集持久化到硬盘上，也可以将它以序列化的Java对象形式（节省空间）持久化到内存中，还可以将这个数据集在节点之间复制。这些存储级别都是通过向`persist()`传递一个`StorageLevel`对象（[Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.storage.StorageLevel), [Java](http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html), [Python](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.StorageLevel))来设置的。``cache（）``方法是使用默认存储级别的缩写，即``StorageLevel.MEMORY_ONLY``（在内存中存储反序列化的对象）。 全套存储级别为：

| 存储级别                                   | 含义                                       |
| -------------------------------------- | ---------------------------------------- |
| MEMORY_ONLY                            | 将RDD作为反序列化的Java对象存储在JVM中。 如果RDD不适合内存存储，某些分区将不会被缓存，每次需要时都会重新计算。 这是默认级别。 |
| MEMORY_AND_DISK                        | 将RDD作为反序列化的Java对象存储在JVM中。 如果RDD不适合内存存储，存储不适合磁盘的分区，并在需要时从那里读取。 |
| MEMORY_ONLY_SER (Java and Scala)       | Store RDD as *serialized* Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a [fast serializer](http://spark.apache.org/docs/latest/tuning.html), but more CPU-intensive to read. |
| MEMORY_AND_DISK_SER (Java and Scala)   | Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed. |
| DISK_ONLY                              | 只存储在磁盘                                   |
| MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. | 和上面一样但是复制在2个节点上                          |
| OFF_HEAP (experimental)                | 与MEMORY_ONLY_SER类似，但将数据存储在堆内存中。 这需要启用堆内存。 |

注意：**在Python中，储存的对象永远是通过Pickle库序列化过的，所以设不设置序列化级别不会产生影响。**

*The available storage levels in Python include MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2, DISK_ONLY, and DISK_ONLY_2.*

Spark还会在shuffle操作（比如`reduceByKey`）中自动储存中间数据，即使用户没有调用`persist`。这是为了防止在shuffle过程中某个节点出错而导致的全盘重算。不过如果用户打算复用某些结果RDD，我们仍然建议用户对结果RDD手动调用`persist`，而不是依赖自动持久化机制。

##### 应该选择哪个存储级别？

Spark的存储级别是为了提供内存使用与CPU效率之间的不同取舍平衡程度。我们建议用户通过考虑以下流程来选择合适的存储级别：

- 如果你的RDD很适合默认的级别（MEMORY_ONLY）,那么久使用默认级别吧。这是CPU最高效运行的选择，能够让RDD上的操作以最快速度运行。

- 否则，试试MEMORY_ONLY_SER选项并且[选择一个快的序列化库](http://spark.apache.org/docs/latest/tuning.html)来使对象的空间利用率更高，同时尽量保证访问速度足够快。

- 不要往硬盘上持久化，除非重算数据集的过程代价确实很昂贵，或者这个过程过滤了巨量的数据。否则，重新计算分片有可能跟读硬盘速度一样快。

- 如果你希望快速的错误恢复（比如用Spark来处理web应用的请求），使用复制级别。所有的存储级别都提供了重算丢失数据的完整容错机制，但是复制一份副本能省去等待重算的时间。

- 在大内存或多应用的环境中，处于实验中的``OFF_HEAP``

  模式有诸多优点：

  - 这个模式允许多个执行者共享Tachyon中的同一个内存池
  - 这个模式显著降低了垃圾回收的花销。
  - 在某一个执行者个体崩溃之后缓存的数据不会丢失。

##### 删除数据

Spark会自动监视每个节点的缓存使用同时使用LRU算法丢弃旧数据分片。如果你想手动删除某个RDD而不是等待它被自动删除，调用`RDD.unpersist()`方法。



#### 共享变量

通常情况下，当一个函数传递给一个在远程集群节点上运行的Spark操作（比如`map`和`reduce`）时，Spark会对涉及到的变量的所有副本执行这个函数。这些变量会被复制到每个机器上，而且这个过程不会被反馈给驱动程序。通常情况下，在任务之间读写共享变量是很低效的。但是，Spark仍然提供了有限的两种共享变量类型用于常见的使用场景：广播变量和累加器。

##### 广播变量

广播变量允许程序员在每台机器上保持一个只读变量的缓存而不是将一个变量的拷贝传递给各个任务。它们可以被使用，比如，给每一个节点传递一份大输入数据集的拷贝是很低效的。Spark试图使用高效的广播算法来分布广播变量，以此来降低通信花销。

Spark的Action通过一组stage执行，由分散的“shuffle”操作隔开。 Spark自动广播每个阶段任务所需的通用数据。 以这种方式广播的数据以序列化形式进行缓存，并在运行每个任务之前进行反序列化。 这意味着，显式创建广播变量仅在跨多个阶段的任务需要相同数据或者以反序列化格式缓存数据很重要时才有用。

可以通过`SparkContext.broadcast(v)`来从变量v创建一个广播变量。这个广播变量是v的一个包装，同时它的值可以功过调用`value`方法来获得。以下的代码展示了这一点：

``` python
>>> broadcastVar = sc.broadcast([1, 2, 3])
<pyspark.broadcast.Broadcast object at 0x102789f10>

>>> broadcastVar.value
[1, 2, 3]
```

在广播变量被创建之后，在所有函数中都应当使用它来代替原来的变量v，这样就可以保证v在节点之间只被传递一次。另外，v变量在被广播之后不应该再被修改了，这样可以确保每一个节点上储存的广播变量的一致性（如果这个变量后来又被传输给一个新的节点）。

##### 累加器

累加器是在一个相关过程中只能被”累加”的变量，对这个变量的操作可以有效地被并行化。它们可以被用于实现计数器（就像在MapReduce过程中）或求和运算。Spark原生支持对数字类型的累加器，程序员也可以为其他新的类型添加支持。累加器被以一个名字创建之后，会在Spark的UI中显示出来。这有助于了解计算的累进过程（注意：目前Python中不支持这个特性）。

![Accumulators in the Spark UI](http://spark.apache.org/docs/latest/img/spark-webui-accumulators.png)

可以通过`SparkContext.accumulator(v)`来从变量v创建一个累加器。在集群中运行的任务随后可以使用`add`方法或+=操作符（在Scala和Python中）来向这个累加器中累加值。但是，他们不能读取累加器中的值。只有驱动程序可以读取累加器中的值，通过累加器的`value`方法。

以下的代码展示了向一个累加器中累加数组元素的过程：

``` python
>>> accum = sc.accumulator(0)
>>> accum
Accumulator<id=0, value=0>

>>> sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))
...
10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s

>>> accum.value
10
```

```python
#这段代码利用了累加器对int类型的内建支持，程序员可以通过继承AccumulatorParam类来创建自己想要的类型支持。AccumulatorParam的接口提供了两个方法：zero'用于为你的数据类型提供零值；'addInPlace'用于计算两个值得和。比如，假设我们有一个Vector`类表示数学中的向量，我们可以这样写：

class VectorAccumulatorParam(AccumulatorParam):
    def zero(self, initialValue):
        return Vector.zeros(initialValue.size)

    def addInPlace(self, v1, v2):
        v1 += v2
        return v1

# Then, create an Accumulator of this type:
vecAccum = sc.accumulator(Vector(...), VectorAccumulatorParam())
```

累加器仅仅是计算了action内部的更新，Spark保证，每个任务中对累加器的更新操作都只会被运行一次。比如，重启一个任务不会再次更新累加器。在转化过程中，用户应该留意每个任务的更新操作在任务或作业重新运算时是否被执行了超过一次。

累加器不会改变spark的惰性求值，如果累加器在对RDD的操作中被更新了，它们的值只会在启动操作中作为RDD计算过程中的一部分被更新。所以，在一个懒惰的转化操作中调用累加器的更新，并没法保证会被及时运行。比如``map``

``` python
accum = sc.accumulator(0)
def g(x):
    accum.add(x)
    return f(x)
data.map(g)
# Here, accum is still 0 because no actions have caused the `map` to be computed.
依然是0，因为没有执行
```

### 在集群上部署

这个[应用提交指南](http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/launcher/package-summary.html)描述了一个应用被提交到集群上的过程。简而言之，只要你把你的应用打成了JAR包（Java/Scala应用）或.py文件的集合或.zip压缩包(Python应用)，bin/spark-submit脚本会将应用提交到任意支持的集群管理器上。

### 单元测试

Spark对单元测试是友好的，可以与任何流行的单元测试框架相容。你只需要在测试中创建一个`SparkContext`，并如前文所述将master的URL设为local，执行你的程序，最后调用`SparkContext.stop()`来终止运行。请确保你在`finally`块或测试框架的`tearDown`方法中终止了上下文，因为Spark不支持两个上下文在一个程序中同时运行。